{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Clone the repository"
      ],
      "metadata": {
        "id": "Rj9PSUZqJWyY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rg-QDUFZ1eab",
        "outputId": "2f2cd0a4-4dfd-47dd-e184-412bbbc7b593"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'crowddiff'...\n",
            "remote: Enumerating objects: 227, done.\u001b[K\n",
            "remote: Counting objects: 100% (227/227), done.\u001b[K\n",
            "remote: Compressing objects: 100% (201/201), done.\u001b[K\n",
            "remote: Total 227 (delta 59), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (227/227), 36.33 MiB | 12.42 MiB/s, done.\n",
            "Resolving deltas: 100% (59/59), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/AravindS1506/crowddiff.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the requirements. This might produce compatibility errors but the code will still run."
      ],
      "metadata": {
        "id": "KMPAXfhOJg-x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "885rVbLJ1tyE",
        "outputId": "5318e61b-9fd8-4335-aaf1-6aab195c12c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/crowddiff\n",
            "Requirement already satisfied: absl-py==1.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.6.3)\n",
            "Collecting blobfile==2.0.0 (from -r requirements.txt (line 3))\n",
            "  Downloading blobfile-2.0.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting cachetools==5.3.0 (from -r requirements.txt (line 4))\n",
            "  Downloading cachetools-5.3.0-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting certifi==2022.12.7 (from -r requirements.txt (line 5))\n",
            "  Downloading certifi-2022.12.7-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting charset-normalizer==3.0.1 (from -r requirements.txt (line 6))\n",
            "  Downloading charset_normalizer-3.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Collecting contourpy==1.0.6 (from -r requirements.txt (line 7))\n",
            "  Downloading contourpy-1.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting cycler==0.11.0 (from -r requirements.txt (line 8))\n",
            "  Downloading cycler-0.11.0-py3-none-any.whl.metadata (785 bytes)\n",
            "Requirement already satisfied: docker-pycreds==0.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (0.4.0)\n",
            "Collecting einops==0.6.0 (from -r requirements.txt (line 10))\n",
            "  Downloading einops-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting filelock==3.8.2 (from -r requirements.txt (line 11))\n",
            "  Downloading filelock-3.8.2-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting flatbuffers==23.1.21 (from -r requirements.txt (line 12))\n",
            "  Downloading flatbuffers-23.1.21-py2.py3-none-any.whl.metadata (850 bytes)\n",
            "Collecting fonttools==4.38.0 (from -r requirements.txt (line 13))\n",
            "  Downloading fonttools-4.38.0-py3-none-any.whl.metadata (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gast==0.4.0 (from -r requirements.txt (line 14))\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting google-auth==2.16.0 (from -r requirements.txt (line 15))\n",
            "  Downloading google_auth-2.16.0-py2.py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting google-auth-oauthlib==0.4.6 (from -r requirements.txt (line 16))\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: google-pasta==0.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (0.2.0)\n",
            "Collecting grpcio==1.51.1 (from -r requirements.txt (line 18))\n",
            "  Downloading grpcio-1.51.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting h5py==3.8.0 (from -r requirements.txt (line 19))\n",
            "  Downloading h5py-3.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting importlib-metadata==6.0.0 (from -r requirements.txt (line 20))\n",
            "  Downloading importlib_metadata-6.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting joblib==1.2.0 (from -r requirements.txt (line 21))\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting keras==2.11.0 (from -r requirements.txt (line 22))\n",
            "  Downloading keras-2.11.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting kiwisolver==1.4.4 (from -r requirements.txt (line 23))\n",
            "  Downloading kiwisolver-1.4.4-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.3 kB)\n",
            "Collecting libclang==15.0.6.1 (from -r requirements.txt (line 24))\n",
            "  Downloading libclang-15.0.6.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting lxml==4.9.2 (from -r requirements.txt (line 25))\n",
            "  Downloading lxml-4.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting Markdown==3.4.1 (from -r requirements.txt (line 26))\n",
            "  Downloading Markdown-3.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting MarkupSafe==2.1.2 (from -r requirements.txt (line 27))\n",
            "  Downloading MarkupSafe-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting matplotlib==3.6.2 (from -r requirements.txt (line 28))\n",
            "  Downloading matplotlib-3.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting numpy==1.24.0 (from -r requirements.txt (line 29))\n",
            "  Downloading numpy-1.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from -r requirements.txt (line 30))\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from -r requirements.txt (line 31))\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from -r requirements.txt (line 32))\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from -r requirements.txt (line 33))\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: oauthlib==3.2.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 34)) (3.2.2)\n",
            "Collecting opencv-python==4.6.0.66 (from -r requirements.txt (line 35))\n",
            "  Downloading opencv_python-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting opt-einsum==3.3.0 (from -r requirements.txt (line 36))\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting packaging==22.0 (from -r requirements.txt (line 37))\n",
            "  Downloading packaging-22.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting pandas==1.5.2 (from -r requirements.txt (line 38))\n",
            "  Downloading pandas-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting pathtools==0.1.2 (from -r requirements.txt (line 39))\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting Pillow==9.3.0 (from -r requirements.txt (line 40))\n",
            "  Downloading Pillow-9.3.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting protobuf==3.20.1 (from -r requirements.txt (line 41))\n",
            "  Downloading protobuf-3.20.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (698 bytes)\n",
            "Collecting pyasn1==0.4.8 (from -r requirements.txt (line 42))\n",
            "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyasn1-modules==0.2.8 (from -r requirements.txt (line 43))\n",
            "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting pycryptodomex==3.16.0 (from -r requirements.txt (line 44))\n",
            "  Downloading pycryptodomex-3.16.0-cp35-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting pyparsing==3.0.9 (from -r requirements.txt (line 45))\n",
            "  Downloading pyparsing-3.0.9-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 46)) (2.8.2)\n",
            "Collecting pytz==2022.7 (from -r requirements.txt (line 47))\n",
            "  Downloading pytz-2022.7-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: requests-oauthlib==1.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 48)) (1.3.1)\n",
            "Requirement already satisfied: rsa==4.9 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 49)) (4.9)\n",
            "Collecting scikit-learn==1.2.1 (from -r requirements.txt (line 50))\n",
            "  Downloading scikit_learn-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting scipy==1.9.3 (from -r requirements.txt (line 51))\n",
            "  Downloading scipy-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard==2.11.2 (from -r requirements.txt (line 52))\n",
            "  Downloading tensorboard-2.11.2-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting tensorboard-data-server==0.6.1 (from -r requirements.txt (line 53))\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting tensorboard-plugin-wit==1.8.1 (from -r requirements.txt (line 54))\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
            "Collecting tensorflow-estimator==2.11.0 (from -r requirements.txt (line 55))\n",
            "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting tensorflow-io-gcs-filesystem==0.30.0 (from -r requirements.txt (line 56))\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.30.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (14 kB)\n",
            "Collecting termcolor==2.2.0 (from -r requirements.txt (line 57))\n",
            "  Downloading termcolor-2.2.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting threadpoolctl==3.1.0 (from -r requirements.txt (line 58))\n",
            "  Downloading threadpoolctl-3.1.0-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting torch==1.13.1 (from -r requirements.txt (line 59))\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting torchvision==0.14.1 (from -r requirements.txt (line 60))\n",
            "  Downloading torchvision-0.14.1-cp310-cp310-manylinux1_x86_64.whl.metadata (11 kB)\n",
            "Collecting tqdm==4.64.1 (from -r requirements.txt (line 61))\n",
            "  Downloading tqdm-4.64.1-py2.py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.3/57.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Werkzeug==2.2.2 (from -r requirements.txt (line 62))\n",
            "  Downloading Werkzeug-2.2.2-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting wrapt==1.14.1 (from -r requirements.txt (line 63))\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting zipp==3.11.0 (from -r requirements.txt (line 64))\n",
            "  Downloading zipp-3.11.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse==1.6.3->-r requirements.txt (line 2)) (0.44.0)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from astunparse==1.6.3->-r requirements.txt (line 2)) (1.16.0)\n",
            "Collecting urllib3~=1.25 (from blobfile==2.0.0->-r requirements.txt (line 3))\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->-r requirements.txt (line 30)) (75.1.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib==1.3.1->-r requirements.txt (line 48)) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->-r requirements.txt (line 59)) (4.12.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib==1.3.1->-r requirements.txt (line 48)) (3.10)\n",
            "Downloading blobfile-2.0.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-3.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (198 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.8/198.8 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.1/296.1 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
            "Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.8.2-py3-none-any.whl (10 kB)\n",
            "Downloading flatbuffers-23.1.21-py2.py3-none-any.whl (26 kB)\n",
            "Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 kB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading google_auth-2.16.0-py2.py3-none-any.whl (177 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.8/177.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Downloading grpcio-1.51.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h5py-3.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-6.0.0-py3-none-any.whl (21 kB)\n",
            "Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kiwisolver-1.4.4-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-15.0.6.1-py2.py3-none-manylinux2010_x86_64.whl (21.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.5/21.5 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml-4.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (7.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading matplotlib-3.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-22.0-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Pillow-9.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodomex-3.16.0-cp35-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2022.7-py2.py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.4/499.4 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.7/33.7 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.30.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
            "Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.14.1-cp310-cp310-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.7/232.7 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zipp-3.11.0-py3-none-any.whl (6.6 kB)\n",
            "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8793 sha256=7c3aa18e96dff1bee73aba858e7523853dac7d3f8aa289a914b5568e1abd23d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: tensorboard-plugin-wit, pytz, pyasn1, pathtools, libclang, flatbuffers, charset-normalizer, zipp, wrapt, urllib3, tqdm, threadpoolctl, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyparsing, pycryptodomex, pyasn1-modules, protobuf, Pillow, packaging, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, numpy, MarkupSafe, Markdown, lxml, kiwisolver, keras, joblib, grpcio, gast, fonttools, filelock, einops, cycler, certifi, cachetools, Werkzeug, scipy, pandas, opt-einsum, opencv-python, nvidia-cudnn-cu11, importlib-metadata, h5py, google-auth, contourpy, blobfile, torch, scikit-learn, matplotlib, torchvision, google-auth-oauthlib, tensorboard\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2024.2\n",
            "    Uninstalling pytz-2024.2:\n",
            "      Successfully uninstalled pytz-2024.2\n",
            "  Attempting uninstall: pyasn1\n",
            "    Found existing installation: pyasn1 0.6.1\n",
            "    Uninstalling pyasn1-0.6.1:\n",
            "      Successfully uninstalled pyasn1-0.6.1\n",
            "  Attempting uninstall: libclang\n",
            "    Found existing installation: libclang 18.1.1\n",
            "    Uninstalling libclang-18.1.1:\n",
            "      Successfully uninstalled libclang-18.1.1\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 24.3.25\n",
            "    Uninstalling flatbuffers-24.3.25:\n",
            "      Successfully uninstalled flatbuffers-24.3.25\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.0\n",
            "    Uninstalling charset-normalizer-3.4.0:\n",
            "      Successfully uninstalled charset-normalizer-3.4.0\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 3.20.2\n",
            "    Uninstalling zipp-3.20.2:\n",
            "      Successfully uninstalled zipp-3.20.2\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.16.0\n",
            "    Uninstalling wrapt-1.16.0:\n",
            "      Successfully uninstalled wrapt-1.16.0\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.2.3\n",
            "    Uninstalling urllib3-2.2.3:\n",
            "      Successfully uninstalled urllib3-2.2.3\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.5\n",
            "    Uninstalling tqdm-4.66.5:\n",
            "      Successfully uninstalled tqdm-4.66.5\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.5.0\n",
            "    Uninstalling threadpoolctl-3.5.0:\n",
            "      Successfully uninstalled threadpoolctl-3.5.0\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 2.5.0\n",
            "    Uninstalling termcolor-2.5.0:\n",
            "      Successfully uninstalled termcolor-2.5.0\n",
            "  Attempting uninstall: tensorflow-io-gcs-filesystem\n",
            "    Found existing installation: tensorflow-io-gcs-filesystem 0.37.1\n",
            "    Uninstalling tensorflow-io-gcs-filesystem-0.37.1:\n",
            "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.37.1\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.2.0\n",
            "    Uninstalling pyparsing-3.2.0:\n",
            "      Successfully uninstalled pyparsing-3.2.0\n",
            "  Attempting uninstall: pyasn1-modules\n",
            "    Found existing installation: pyasn1_modules 0.4.1\n",
            "    Uninstalling pyasn1_modules-0.4.1:\n",
            "      Successfully uninstalled pyasn1_modules-0.4.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: pillow 10.4.0\n",
            "    Uninstalling pillow-10.4.0:\n",
            "      Successfully uninstalled pillow-10.4.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.1\n",
            "    Uninstalling packaging-24.1:\n",
            "      Successfully uninstalled packaging-24.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: Markdown\n",
            "    Found existing installation: Markdown 3.7\n",
            "    Uninstalling Markdown-3.7:\n",
            "      Successfully uninstalled Markdown-3.7\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.9.4\n",
            "    Uninstalling lxml-4.9.4:\n",
            "      Successfully uninstalled lxml-4.9.4\n",
            "  Attempting uninstall: kiwisolver\n",
            "    Found existing installation: kiwisolver 1.4.7\n",
            "    Uninstalling kiwisolver-1.4.7:\n",
            "      Successfully uninstalled kiwisolver-1.4.7\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.4.2\n",
            "    Uninstalling joblib-1.4.2:\n",
            "      Successfully uninstalled joblib-1.4.2\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.64.1\n",
            "    Uninstalling grpcio-1.64.1:\n",
            "      Successfully uninstalled grpcio-1.64.1\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.6.0\n",
            "    Uninstalling gast-0.6.0:\n",
            "      Successfully uninstalled gast-0.6.0\n",
            "  Attempting uninstall: fonttools\n",
            "    Found existing installation: fonttools 4.54.1\n",
            "    Uninstalling fonttools-4.54.1:\n",
            "      Successfully uninstalled fonttools-4.54.1\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.16.1\n",
            "    Uninstalling filelock-3.16.1:\n",
            "      Successfully uninstalled filelock-3.16.1\n",
            "  Attempting uninstall: einops\n",
            "    Found existing installation: einops 0.8.0\n",
            "    Uninstalling einops-0.8.0:\n",
            "      Successfully uninstalled einops-0.8.0\n",
            "  Attempting uninstall: cycler\n",
            "    Found existing installation: cycler 0.12.1\n",
            "    Uninstalling cycler-0.12.1:\n",
            "      Successfully uninstalled cycler-0.12.1\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2024.8.30\n",
            "    Uninstalling certifi-2024.8.30:\n",
            "      Successfully uninstalled certifi-2024.8.30\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 5.5.0\n",
            "    Uninstalling cachetools-5.5.0:\n",
            "      Successfully uninstalled cachetools-5.5.0\n",
            "  Attempting uninstall: Werkzeug\n",
            "    Found existing installation: Werkzeug 3.0.6\n",
            "    Uninstalling Werkzeug-3.0.6:\n",
            "      Successfully uninstalled Werkzeug-3.0.6\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: opt-einsum\n",
            "    Found existing installation: opt_einsum 3.4.0\n",
            "    Uninstalling opt_einsum-3.4.0:\n",
            "      Successfully uninstalled opt_einsum-3.4.0\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.10.0.84\n",
            "    Uninstalling opencv-python-4.10.0.84:\n",
            "      Successfully uninstalled opencv-python-4.10.0.84\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 8.5.0\n",
            "    Uninstalling importlib_metadata-8.5.0:\n",
            "      Successfully uninstalled importlib_metadata-8.5.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.11.0\n",
            "    Uninstalling h5py-3.11.0:\n",
            "      Successfully uninstalled h5py-3.11.0\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 2.27.0\n",
            "    Uninstalling google-auth-2.27.0:\n",
            "      Successfully uninstalled google-auth-2.27.0\n",
            "  Attempting uninstall: contourpy\n",
            "    Found existing installation: contourpy 1.3.0\n",
            "    Uninstalling contourpy-1.3.0:\n",
            "      Successfully uninstalled contourpy-1.3.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.0+cu121\n",
            "    Uninstalling torch-2.5.0+cu121:\n",
            "      Successfully uninstalled torch-2.5.0+cu121\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.5.2\n",
            "    Uninstalling scikit-learn-1.5.2:\n",
            "      Successfully uninstalled scikit-learn-1.5.2\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.7.1\n",
            "    Uninstalling matplotlib-3.7.1:\n",
            "      Successfully uninstalled matplotlib-3.7.1\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.20.0+cu121\n",
            "    Uninstalling torchvision-0.20.0+cu121:\n",
            "      Successfully uninstalled torchvision-0.20.0+cu121\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.19 requires numpy>=1.24.4, but you have numpy 1.24.0 which is incompatible.\n",
            "albumentations 1.4.20 requires numpy>=1.24.4, but you have numpy 1.24.0 which is incompatible.\n",
            "albumentations 1.4.20 requires scipy>=1.10.0, but you have scipy 1.9.3 which is incompatible.\n",
            "bigframes 1.24.0 requires matplotlib>=3.7.1, but you have matplotlib 3.6.2 which is incompatible.\n",
            "bigframes 1.24.0 requires pandas>=1.5.3, but you have pandas 1.5.2 which is incompatible.\n",
            "bigframes 1.24.0 requires scikit-learn>=1.2.2, but you have scikit-learn 1.2.1 which is incompatible.\n",
            "bokeh 3.4.3 requires contourpy>=1.2, but you have contourpy 1.0.6 which is incompatible.\n",
            "chex 0.1.87 requires numpy>=1.24.1, but you have numpy 1.24.0 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.2 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.10 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-api-core 2.19.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-aiplatform 1.70.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.15.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.27.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigtable 2.26.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-datastore 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-firestore 2.16.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-functions 1.16.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-iam 2.16.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-language 2.13.4 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-pubsub 2.25.0 requires grpcio<2.0dev,>=1.51.3, but you have grpcio 1.51.1 which is incompatible.\n",
            "google-cloud-pubsub 2.25.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-resource-manager 1.13.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-translate 3.15.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-colab 1.0.0 requires google-auth==2.27.0, but you have google-auth 2.16.0 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.2 which is incompatible.\n",
            "googleapis-common-protos 1.65.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "grpc-google-iam-v1 0.13.1 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "ibis-framework 9.2.0 requires pandas<3,>=1.5.3, but you have pandas 1.5.2 which is incompatible.\n",
            "jax 0.4.33 requires scipy>=1.10, but you have scipy 1.9.3 which is incompatible.\n",
            "jaxlib 0.4.33 requires scipy>=1.10, but you have scipy 1.9.3 which is incompatible.\n",
            "kaggle 1.6.17 requires certifi>=2023.7.22, but you have certifi 2022.12.7 which is incompatible.\n",
            "langchain-core 0.3.13 requires packaging<25,>=23.2, but you have packaging 22.0 which is incompatible.\n",
            "mizani 0.11.4 requires pandas>=2.1.0, but you have pandas 1.5.2 which is incompatible.\n",
            "pandas-gbq 0.23.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "plotnine 0.13.6 requires matplotlib>=3.7.0, but you have matplotlib 3.6.2 which is incompatible.\n",
            "plotnine 0.13.6 requires pandas<3.0.0,>=2.1.0, but you have pandas 1.5.2 which is incompatible.\n",
            "pytensor 2.25.5 requires filelock>=3.15, but you have filelock 3.8.2 which is incompatible.\n",
            "seaborn 0.13.2 requires numpy!=1.24.0,>=1.20, but you have numpy 1.24.0 which is incompatible.\n",
            "tensorflow 2.17.0 requires flatbuffers>=24.3.25, but you have flatbuffers 23.1.21 which is incompatible.\n",
            "tensorflow 2.17.0 requires h5py>=3.10.0, but you have h5py 3.8.0 which is incompatible.\n",
            "tensorflow 2.17.0 requires keras>=3.2.0, but you have keras 2.11.0 which is incompatible.\n",
            "tensorflow 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.1 which is incompatible.\n",
            "tensorflow 2.17.0 requires tensorboard<2.18,>=2.17, but you have tensorboard 2.11.2 which is incompatible.\n",
            "tensorflow-metadata 1.16.1 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 3.20.1 which is incompatible.\n",
            "torchaudio 2.5.0+cu121 requires torch==2.5.0, but you have torch 1.13.1 which is incompatible.\n",
            "xarray 2024.9.0 requires packaging>=23.1, but you have packaging 22.0 which is incompatible.\n",
            "xarray 2024.9.0 requires pandas>=2.1, but you have pandas 1.5.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Markdown-3.4.1 MarkupSafe-2.1.2 Pillow-9.3.0 Werkzeug-2.2.2 blobfile-2.0.0 cachetools-5.3.0 certifi-2022.12.7 charset-normalizer-3.0.1 contourpy-1.0.6 cycler-0.11.0 einops-0.6.0 filelock-3.8.2 flatbuffers-23.1.21 fonttools-4.38.0 gast-0.4.0 google-auth-2.16.0 google-auth-oauthlib-0.4.6 grpcio-1.51.1 h5py-3.8.0 importlib-metadata-6.0.0 joblib-1.2.0 keras-2.11.0 kiwisolver-1.4.4 libclang-15.0.6.1 lxml-4.9.2 matplotlib-3.6.2 numpy-1.24.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 opencv-python-4.6.0.66 opt-einsum-3.3.0 packaging-22.0 pandas-1.5.2 pathtools-0.1.2 protobuf-3.20.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycryptodomex-3.16.0 pyparsing-3.0.9 pytz-2022.7 scikit-learn-1.2.1 scipy-1.9.3 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-estimator-2.11.0 tensorflow-io-gcs-filesystem-0.30.0 termcolor-2.2.0 threadpoolctl-3.1.0 torch-1.13.1 torchvision-0.14.1 tqdm-4.64.1 urllib3-1.26.20 wrapt-1.14.1 zipp-3.11.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "certifi",
                  "cycler",
                  "google",
                  "kiwisolver",
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy"
                ]
              },
              "id": "f8111204ce8a4801a365b1892b805a3f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "%cd crowddiff\n",
        "!pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refreshing might default the path to content, so ensure that the base is in the crowddiff folder"
      ],
      "metadata": {
        "id": "mNu-I4i-JkEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd crowddiff"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IyYEn8OD_2O",
        "outputId": "30cfaa13-1a4b-472d-8541-4b5f08c7d32c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/crowddiff\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install mpi4y, this command works in google colab"
      ],
      "metadata": {
        "id": "Jy7sXmFyJrxe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "32AoF-CbXdGQ",
        "outputId": "69ba1822-11a6-4910-9165-c854a8412400"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,091 kB]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Ign:9 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,450 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,162 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,305 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,432 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,665 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,227 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,389 kB]\n",
            "Get:21 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,606 kB]\n",
            "Fetched 26.7 MB in 5s (5,759 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libopenmpi-dev is already the newest version (4.1.2-2ubuntu1).\n",
            "openmpi-bin is already the newest version (4.1.2-2ubuntu1).\n",
            "openmpi-bin set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 50 not upgraded.\n",
            "Collecting mpi4py\n",
            "  Downloading mpi4py-4.0.1.tar.gz (466 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m466.2/466.2 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-4.0.1-cp310-cp310-linux_x86_64.whl size=4266348 sha256=1129f20589373802a359b02a69bf614740a4cd7d1296bc9759c3a73d7f876e0c\n",
            "  Stored in directory: /root/.cache/pip/wheels/3c/ca/13/13218a83854023ccec184e3af482f0f038b434aa32c19afee8\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-4.0.1\n"
          ]
        }
      ],
      "source": [
        "!apt-get update && apt-get install -y libopenmpi-dev openmpi-bin\n",
        "!pip install mpi4py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess jhu crowd dataset"
      ],
      "metadata": {
        "id": "Jm_6BCnsJx4U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SkChmuVH21v8",
        "outputId": "c4fb8e8c-b47f-4fa6-f5c9-6a711d3c9a42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14\n",
            "[0.6186935068229407]\n",
            "0\n",
            "{}\n"
          ]
        }
      ],
      "source": [
        "!python cc_utils/preprocess_jhu.py --data_dir  dataset --output_dir out_data --dataset jhu_crowd --mode test --image_size 256 --ndevices 1 --sigma '0.5'  --kernel_size '3'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the model and store it in the model folder"
      ],
      "metadata": {
        "id": "LO8RG1UaJ0bT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "XfU3z38LUz59",
        "outputId": "c52d5a89-0b6e-43d2-de21-735654dee072"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/crowddiff/model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1dLEjaZqw9bxQm2sUU4I6YXDnFfyEHl8p\n",
            "From (redirected): https://drive.google.com/uc?id=1dLEjaZqw9bxQm2sUU4I6YXDnFfyEHl8p&confirm=t&uuid=38187328-70e8-4dd0-96d7-f9a63d3b92ee\n",
            "To: /content/crowddiff/model/demo.pt\n",
            "100%|██████████| 1.24G/1.24G [00:39<00:00, 31.9MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'demo.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "%mkdir model\n",
        "%cd model\n",
        "import gdown\n",
        "\n",
        "# Replace 'YOUR_FILE_ID' with the actual ID from your Google Drive link\n",
        "file_id = '1dLEjaZqw9bxQm2sUU4I6YXDnFfyEHl8p'\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', 'demo.pt', quiet=False)\n",
        "\n",
        "\n",
        "# Replace 'FILE_ID' with the actual ID from your Google Drive link"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the diffusion model code"
      ],
      "metadata": {
        "id": "4Lr6OqlxJ3s9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../\n",
        "%cp -r \"cc_utils\" \"scripts\""
      ],
      "metadata": {
        "id": "nJ2iG3UcJ65l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c6541b0-121f-4e68-8876-0a27d73f1bca"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/crowddiff\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2ROPaAjhRwBt",
        "outputId": "c56ca1c2-6d66-4c9a-a7a5-e3e438968eb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging to results\n",
            "creating model...\n",
            "loading data...\n",
            "creating samples...\n",
            "100% 100/100 [00:20<00:00,  4.87it/s]\n",
            "100% 100/100 [00:15<00:00,  6.46it/s]\n",
            " 16% 16/100 [00:02<00:14,  6.00it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/crowddiff/scripts/super_res_sample.py\", line 496, in <module>\n",
            "    main()\n",
            "  File \"/content/crowddiff/scripts/super_res_sample.py\", line 115, in main\n",
            "    samples = diffusion.p_sample_loop(\n",
            "  File \"/content/crowddiff/scripts/guided_diffusion/gaussian_diffusion.py\", line 499, in p_sample_loop\n",
            "    for sample in self.p_sample_loop_progressive(\n",
            "  File \"/content/crowddiff/scripts/guided_diffusion/gaussian_diffusion.py\", line 596, in p_sample_loop_progressive\n",
            "    out = self.p_sample(\n",
            "  File \"/content/crowddiff/scripts/guided_diffusion/gaussian_diffusion.py\", line 439, in p_sample\n",
            "    out = self.p_mean_variance(\n",
            "  File \"/content/crowddiff/scripts/guided_diffusion/respace.py\", line 92, in p_mean_variance\n",
            "    return super().p_mean_variance(self._wrap_model(model), *args, **kwargs)\n",
            "  File \"/content/crowddiff/scripts/guided_diffusion/gaussian_diffusion.py\", line 284, in p_mean_variance\n",
            "    min_log = _extract_into_tensor(\n",
            "  File \"/content/crowddiff/scripts/guided_diffusion/gaussian_diffusion.py\", line 1006, in _extract_into_tensor\n",
            "    res = th.from_numpy(arr).to(device=timesteps.device)[timesteps].float()\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "\n",
        "DATA_DIR=\"--data_dir out_data/jhu_crowd/part_1/test/\"\n",
        "LOG_DIR=\"--log_dir results --model_path model/demo.pt\"\n",
        "TRAIN_FLAGS=\"--normalizer 0.8 --pred_channels 1 --batch_size 1 --per_samples 1\"\n",
        "MODEL_FLAGS=\"--attention_resolutions 32,16,8 --class_cond False --diffusion_steps 1000 --large_size 256  --small_size 256 --learn_sigma True --noise_schedule linear --num_channels 192 --num_head_channels 64 --num_res_blocks 2 --resblock_updown True --use_fp16 True --use_checkpoint True\"\n",
        "!CUDA_VISIBLE_DEVICES=0 python scripts/super_res_sample.py $DATA_DIR $LOG_DIR $TRAIN_FLAGS $MODEL_FLAGS"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an additional code for visualization of the generated maps"
      ],
      "metadata": {
        "id": "cUJy65SNKAdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR=\"--data_dir out_data/shtech_A/part_1/test/\"\n",
        "LOG_DIR=\"--log_dir results --model_path model/demo.pt\"\n",
        "TRAIN_FLAGS=\"--normalizer 0.8 --pred_channels 1 --batch_size 1 --per_samples 1\"\n",
        "MODEL_FLAGS=\"--attention_resolutions 32,16,8 --class_cond False --diffusion_steps 1000 --large_size 256  --small_size 256 --learn_sigma True --noise_schedule linear --num_channels 192 --num_head_channels 64 --num_res_blocks 2 --resblock_updown True --use_fp16 True --use_checkpoint True\"\n",
        "!CUDA_VISIBLE_DEVICES=0 python scripts/super_res_sample.py $DATA_DIR $LOG_DIR $TRAIN_FLAGS $MODEL_FLAGS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUdYHhavIldA",
        "outputId": "e9613b4c-326e-49bf-fdcd-d4fe349946fb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging to results\n",
            "creating model...\n",
            "loading data...\n",
            "creating samples...\n",
            "100% 100/100 [00:16<00:00,  6.07it/s]\n",
            "100% 100/100 [00:16<00:00,  6.16it/s]\n",
            "100% 100/100 [00:16<00:00,  5.94it/s]\n",
            "Img 1 and Count 0\n",
            "MAE: 4.333333333333333\n",
            "MSE: 4.654746681256314\n",
            "sampling complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generated maps are stored in pred_density.png and can be visualized after running the merge file. The output is stored in contour_merge"
      ],
      "metadata": {
        "id": "olJ8C2HiKH6t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ie8StM-7IUh8",
        "outputId": "292820df-fdd5-46c6-a8f4-bf2c30ebf668"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "92\n"
          ]
        }
      ],
      "source": [
        "!python merge.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HUS2ADQ3NopC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
